{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15829a43",
      "metadata": {
        "id": "15829a43"
      },
      "source": [
        "- <a href='#text cleaning'>text cleaning</a>\n",
        "    - <a href='#stop words in EN'>1.1.  stop words in EN</a>\n",
        "    - <a href='#punctuation'>1.2.  Punctuation</a>\n",
        "    - <a href='#stemming using nltk'>1.3.  stemming using nltk</a>\n",
        "    - <a href='#stemming in AR'>1.4.  stemming in AR</a>\n",
        "    - <a href='#lemmatization using nltk'>1.5.  lemmatization using nltk</a>\n",
        "    - <a href='#lemmatization in AR'>1.6.  lemmatization in AR</a>\n",
        "    - <a href='#special arabic cleaning functions'>1.6.  special arabic cleaning functions</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d8c85fa",
      "metadata": {
        "id": "4d8c85fa"
      },
      "source": [
        "<b>\n",
        "<a id='text cleaning'></a>\n",
        "<font size=\"7\">text cleaning</font>\n",
        "</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25e3edb8",
      "metadata": {
        "id": "25e3edb8"
      },
      "source": [
        "<b>\n",
        "<a id='stop words in EN'></a>\n",
        "<font size=\"5\">stop words in EN</font>\n",
        "</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "172ae06b",
      "metadata": {
        "id": "172ae06b",
        "outputId": "8df33c3c-d81a-4dde-cbd5-7ac505f2485d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine\n",
        "has been programmed to ignore, both when indexing entries for searching and when retrieving them as\n",
        "\n",
        "the result of a search query.\n",
        "'''\n",
        "#NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e71dbbb8",
      "metadata": {
        "id": "e71dbbb8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f5320f9",
      "metadata": {
        "id": "3f5320f9",
        "outputId": "54f5df05-1b09-4d6f-f235-38a007223323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n",
            "sample|sentence|,|showing|stop|words|filtration|.\n"
          ]
        }
      ],
      "source": [
        "#Removing stop words with NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sent = \"\"\"This is a sample sentence,\n",
        "                  showing off the stop words filtration.\"\"\"\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "# converts the words in word_tokens to lower case and then checks whether\n",
        "#they are present in stop_words or not\n",
        "filtered_sentence = []\n",
        "for w in word_tokens:\n",
        "    if w.lower() not in stop_words:\n",
        "        filtered_sentence.append(w.lower())\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)\n",
        "print('|'.join(filtered_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a646798d",
      "metadata": {
        "id": "a646798d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87f19adb",
      "metadata": {
        "id": "87f19adb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ecfe330",
      "metadata": {
        "id": "3ecfe330"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "29bf645d",
      "metadata": {
        "id": "29bf645d"
      },
      "source": [
        "<b>\n",
        "<a id='stop words in AR'></a>\n",
        "<font size=\"5\">stop words in AR</font>\n",
        "</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f495a7f",
      "metadata": {
        "id": "1f495a7f",
        "outputId": "7863477f-2cc4-4fcc-9157-f0a711b394fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "243\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'اللتين | إليك | عليك | حتى | على | أو | كل | له | وإذ | تينك | كي | اللتيا | أين | حاشا | كليهما | كأن | إنه | لسنا | مع | لستن | كيف | هاتي | بيد | في | كم | لكم | بك | هذا | أوه | اللواتي | بمن | منذ | اللاتي | إما | بكما | بهن | أكثر | لكي | ذا | هاته | هاتين | هاتان | وما | آها | بي | ها | لدى | بنا | هاك | تي | سوى | ذلكن | شتان | أولئك | بخ | فمن | أقل | عليه | بكن | ألا | تلك | لكما | متى | هذان | كليكما | إنا | لن | مذ | إيه | ولا | قد | إليكم | ولو | أما | أنت | ذلكما | لك | بل | ليت | آي | كأي | التي | ذينك | ومن | بكم | لعل | أنا | أينما | حين | لها | ذات | خلا | آه | هناك | لهن | أنتما | ذي | لكنما | سوف | نحو | هكذا | هيهات | اللذين | هاهنا | ولكن | عسى | فيما | إذن | هما | هل | ذواتي | فإذا | أنتم | كلا | فيها | ذوا | إذما | منه | اللذان | أنى | كأنما | يا | إن | فإن | عدا | إذا | كيفما | ذانك | إي | كلما | ثمة | لوما | منها | كلاهما | كما | لاسيما | عل | هيت | وإذا | كذا | ذان | أولاء | ذاك | حبذا | ريث | اللتان | نعم | إليكن | فلا | حيث | هؤلاء | إليكما | به | عما | لهم | لست | ليستا | عن | بس | هي | أي | ماذا | والذي | تلكم | لستم | بما | بماذا | ته | بعد | ليست | هذه | هم | ليسوا | بين | إلا | الذين | هن | ذلكم | دون | هلا | لم | إنما | أيها | فيم | غير | لسن | فيه | ذه | هيا | بهم | نحن | لهما | لئن | إلى | لي | بلى | حيثما | ذواتا | لو | لما | بهما | عند | كلتا | ذو | لنا | مما | أم | ممن | لا | هذي | بعض | ليس | ما | هذين | مه | لستما | هنا | ليسا | لكن | وهو | هنالك | أف | إذ | كيت | لولا | اللائي | مهما | وإن | كأين | تلكما | ثم | كذلك | والذين | هو | ذلك | أن | بها | لكيلا | أنتن | الذي | من | ذين | تين'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Ar_SW1 = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
        "print(len(Ar_SW1))\n",
        "' | '.join(Ar_SW1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4d8a0b",
      "metadata": {
        "id": "1f4d8a0b"
      },
      "source": [
        "<b>\n",
        "<a id='punctuation'></a>\n",
        "<font size=\"5\">punctuation</font>\n",
        "</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0de2eaf",
      "metadata": {
        "id": "d0de2eaf",
        "outputId": "3dab80e3-123c-4698-bb37-8d32a405ca56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "source": [
        "from string import punctuation\n",
        "\n",
        "punctuation = list(punctuation)\n",
        "print(punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6663ff52",
      "metadata": {
        "id": "6663ff52"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75111fc0",
      "metadata": {
        "id": "75111fc0",
        "outputId": "5c5a21fd-5eb0-4d1c-a74e-abc03dde4050"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['My', 'Email', 'address', 'is', ':', 'taneshbalodi8', '@', 'gmail.com', '.']\n",
            "['My', 'Email', 'address', 'is', 'taneshbalodi8', 'gmail.com']\n"
          ]
        }
      ],
      "source": [
        "#removing punctuation with tokenization\n",
        "example_sent = \"\"\"My Email address is: taneshbalodi8@gmail.com.\"\"\"\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in punctuation:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68cf89ea",
      "metadata": {
        "id": "68cf89ea"
      },
      "source": [
        "<b>\n",
        "<a id='stemming using nltk'></a>\n",
        "<font size=\"5\">stemming using nltk</font>\n",
        "</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04357e39",
      "metadata": {
        "id": "04357e39"
      },
      "outputs": [],
      "source": [
        "#Stemming is the process of producing morphological variants of a root/base word.\n",
        "#it is used to normalize text and make it easier to process\n",
        "#اداه تسمح بتجريد اى كلمه من جميع الاضافات التى فيها والعوده للمصدر الاصلى لها\n",
        "#(plays,played,playing,player) كلها تعود الى play\n",
        "#يعنى ممكن نقول انها بتحذف السوابق واللواحق للكلمه\n",
        "#It is used to determine domain vocabularies in domain analysis.\n",
        "#Stemming and lemmatization are methods used by search engines and chatbots to analyze the meaning behind a word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f52dc997",
      "metadata": {
        "id": "f52dc997",
        "outputId": "e9fe0f4b-4060-461b-9952-a4d8a317b8b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "program  :  program\n",
            "programs  :  program\n",
            "programmer  :  programm\n",
            "programming  :  program\n",
            "programmers  :  programm\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# choose some words to be stemmed\n",
        "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
        "\n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))\n",
        "#lancaster stemmer?\n",
        "#regexp stemmer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47dedf75",
      "metadata": {
        "id": "47dedf75",
        "outputId": "6ba3a4de-2538-4beb-f0d5-f9f6c588640e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Programmers  :  programm\n",
            "program  :  program\n",
            "with  :  with\n",
            "programming  :  program\n",
            "languages  :  languag\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Programmers program with programming languages\"\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c7fab57",
      "metadata": {
        "id": "2c7fab57",
        "outputId": "25bfdb73-9ee3-4d97-c79a-ae0fe412696e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generous ---> generous\n",
            "generate ---> generat\n",
            "generously ---> generous\n",
            "generation ---> generat\n"
          ]
        }
      ],
      "source": [
        "# snowballstemmer is somewhat faster and more logical than the original Porter Stemmer.\n",
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language='english')\n",
        "words = ['generous','generate','generously','generation']\n",
        "for word in words:\n",
        "    print(word,\"--->\",snowball.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19f156a9",
      "metadata": {
        "id": "19f156a9"
      },
      "source": [
        "<b>\n",
        "<a id='stemming in AR'></a>\n",
        "<font size=\"5\">stemming in AR</font>\n",
        "</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6643e7dd",
      "metadata": {
        "id": "6643e7dd",
        "outputId": "f1b5a3f7-20c6-4989-d57a-2d65f721ecdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "الجري --> الجري\n",
            "تجري --> تجري\n",
            "يجرون --> يجرون\n",
            "جري --> جري\n",
            "يجري --> يجري\n"
          ]
        }
      ],
      "source": [
        "#stemming is very weak in arabic\n",
        "words = ['الجري','تجري','يجرون','جري','يجري']\n",
        "for word in words:\n",
        "    print(word+' --> '+ps.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c12266",
      "metadata": {
        "id": "60c12266",
        "outputId": "1c217dcc-dea0-41bc-ba29-901e9f0c9811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "الجري --> الجر\n",
            "تجري --> تجر\n",
            "يجرون --> يجرو\n",
            "جري --> جر\n",
            "يجري --> يجر\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "s_stemmer = SnowballStemmer(language='arabic')\n",
        "\n",
        "words = ['الجري','تجري','يجرون','جري','يجري']\n",
        "for word in words:\n",
        "    print(word+' --> '+s_stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad08e62",
      "metadata": {
        "id": "8ad08e62",
        "outputId": "0c70dbc5-6d15-48ff-bd63-bf8b1682b517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "جري\n",
            "تجر\n",
            "يجر\n",
            "جري\n",
            "يجر\n",
            "علم\n"
          ]
        }
      ],
      "source": [
        "#ISRIStemmer returns Arabic root for the given token\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "st = ISRIStemmer()\n",
        "words = ['الجري','تجري','يجرون','جري','يجري']\n",
        "\n",
        "for word in words :\n",
        "    print(st.stem(word))\n",
        "print (st.stem('اعلاميون'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d5f67b1",
      "metadata": {
        "id": "5d5f67b1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cd8cb23",
      "metadata": {
        "id": "1cd8cb23"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a167dc33",
      "metadata": {
        "id": "a167dc33"
      },
      "source": [
        "<b>\n",
        "<a id='lemmatization using nltk'></a>\n",
        "<font size=\"5\">lemmatization using nltk</font>\n",
        "</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b950456",
      "metadata": {
        "id": "1b950456"
      },
      "outputs": [],
      "source": [
        "#Lemmatization is similar to stemming but it brings context to the words.\n",
        "#So it links words with similar meanings to one word.\n",
        "#اكثر قوة وفعاليه لانها مش بتكتفى انها تحذف الزوائد من الكلمات ولكن بتبحث ف معنى واصل الكلمه\n",
        "#for example been and was has a lemma of be\n",
        "#for example meeting word has a root of meet but it may be a noun in the sentence\n",
        "#so the root is meeting not meet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd4c48b1",
      "metadata": {
        "id": "fd4c48b1",
        "outputId": "130b396a-11c3-48f7-fe2a-d075cd68c08c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nExamples of lemmatization:\\n\\n-> rocks : rock\\n-> corpora : corpus\\n-> better : good\\n'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Examples of lemmatization:\n",
        "\n",
        "-> rocks : rock\n",
        "-> corpora : corpus\n",
        "-> better : good\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72c884e0",
      "metadata": {
        "id": "72c884e0",
        "outputId": "edac1fab-b1be-4017-be36-ad65f94b37d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "\n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e7ce2e",
      "metadata": {
        "id": "17e7ce2e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ed8186",
      "metadata": {
        "id": "f6ed8186",
        "outputId": "9560055a-2dfd-4d5a-c009-720ddd93613e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is\n",
            "wa\n",
            "be\n",
            "been\n",
            "are\n",
            "were\n"
          ]
        }
      ],
      "source": [
        "words = [\"is\",\"was\",\"be\",\"been\",\"are\",\"were\"]\n",
        "\n",
        "for word in words :\n",
        "    print(lemmatizer.lemmatize(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71a52503",
      "metadata": {
        "id": "71a52503"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa911ce",
      "metadata": {
        "id": "2fa911ce",
        "outputId": "8bf25adc-affc-4f0f-dc14-19c9ad19d0c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "be\n",
            "be\n",
            "be\n",
            "be\n",
            "be\n",
            "be\n"
          ]
        }
      ],
      "source": [
        "#what if we put them as verb\n",
        "words = [\"is\",\"was\",\"be\",\"been\",\"are\",\"were\"]\n",
        "for word in words :\n",
        "    print(lemmatizer.lemmatize(word,'v'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6938bc6",
      "metadata": {
        "id": "c6938bc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0542e416",
      "metadata": {
        "id": "0542e416"
      },
      "source": [
        "<b>\n",
        "<a id='lemmatization in AR'></a>\n",
        "<font size=\"5\">lemmatization in AR</font>\n",
        "</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63175bb9",
      "metadata": {
        "id": "63175bb9",
        "outputId": "a16dd145-470d-418c-e8dd-9dd584a40d99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "الجري\n",
            "تجري\n",
            "يجرون\n",
            "جري\n",
            "يجري\n"
          ]
        }
      ],
      "source": [
        "words = ['الجري','تجري','يجرون','جري','يجري']\n",
        "\n",
        "for word in words :\n",
        "    print(lemmatizer.lemmatize(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc535da0",
      "metadata": {
        "id": "cc535da0",
        "outputId": "c24af7ff-fc0b-4551-9387-9364da32b9ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting qalsadi\n",
            "  Downloading qalsadi-0.4.6-py3-none-any.whl (256 kB)\n",
            "Collecting pyarabic>=0.6.7\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "Collecting tashaphyne>=0.3.4.1\n",
            "  Downloading Tashaphyne-0.3.6-py3-none-any.whl (251 kB)\n",
            "Collecting naftawayh>=0.3\n",
            "  Using cached Naftawayh-0.4-py3-none-any.whl (332 kB)\n",
            "Collecting arramooz-pysqlite>=0.3\n",
            "  Downloading arramooz_pysqlite-0.4.1-py3-none-any.whl (14.5 MB)\n",
            "Collecting pickledb>=0.9.2\n",
            "  Using cached pickleDB-0.9.2.tar.gz (3.7 kB)\n",
            "Collecting alyahmor>=0.1\n",
            "  Downloading alyahmor-0.2-py3-none-any.whl (65 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in c:\\users\\elrowad\\anaconda3\\lib\\site-packages (from qalsadi) (1.15.0)\n",
            "Collecting libqutrub>=1.2.3\n",
            "  Using cached libqutrub-1.2.4.1-py3-none-any.whl (138 kB)\n",
            "Collecting Arabic-Stopwords>=0.3\n",
            "  Downloading Arabic_Stopwords-0.4.3-py3-none-any.whl (360 kB)\n",
            "Requirement already satisfied: future>=0.16.0 in c:\\users\\elrowad\\anaconda3\\lib\\site-packages (from qalsadi) (0.18.2)\n",
            "Building wheels for collected packages: pickledb\n",
            "  Building wheel for pickledb (setup.py): started\n",
            "  Building wheel for pickledb (setup.py): finished with status 'done'\n",
            "  Created wheel for pickledb: filename=pickleDB-0.9.2-py3-none-any.whl size=4270 sha256=df17ced0b251c3d25440d56d6eb75387ec876d07548e792f786589bddc94e97e\n",
            "  Stored in directory: c:\\users\\elrowad\\appdata\\local\\pip\\cache\\wheels\\88\\91\\d4\\ef2e6a46ad2bc41f9cfad35fa2db5b34357a5e4da67c385ffa\n",
            "Successfully built pickledb\n",
            "Installing collected packages: pyarabic, tashaphyne, libqutrub, arramooz-pysqlite, Arabic-Stopwords, pickledb, naftawayh, alyahmor, qalsadi\n",
            "Successfully installed Arabic-Stopwords-0.4.3 alyahmor-0.2 arramooz-pysqlite-0.4.1 libqutrub-1.2.4.1 naftawayh-0.4 pickledb-0.9.2 pyarabic-0.6.15 qalsadi-0.4.6 tashaphyne-0.3.6\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install qalsadi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88619361",
      "metadata": {
        "id": "88619361",
        "outputId": "0c5d21e1-623d-42a9-fb33-e3e962357e5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "جرة تجرة جرى جرة جرى \n",
            "يقل يقل تقل قول\n"
          ]
        }
      ],
      "source": [
        "import qalsadi.lemmatizer\n",
        "lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
        "words = ['الجري','تجري','يجرون','جري','يجري']\n",
        "\n",
        "for word in words :\n",
        "    print(lemmer.lemmatize(word),end=' ')\n",
        "print()\n",
        "print (st.stem('يقول'),end=' ')\n",
        "print (st.stem('يقولون'),end=' ')\n",
        "print (st.stem('تقول'),end=' ')\n",
        "print (st.stem('مقوله'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec964e24",
      "metadata": {
        "id": "ec964e24",
        "outputId": "62d8da0a-d37d-41e9-c135-53fd8e292340"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['هل', 'احتاج', 'إلى', 'ترجمة', 'كي', 'تف', 'خطاب', 'ملك', '؟', 'لغة', '\"', 'كلاسيكي', '\"(', 'فصحى', ')', 'موجود', 'في', 'كل', 'لغة', 'ذلك', 'لغة', '\"', 'دارج', '\"..', 'فرنسة', 'التي', 'درس', 'في', 'مدرس', 'ليست', 'فرنسة', 'التي', 'استخدم', 'ناس', 'في', 'شوارع', 'باريس', '..', 'ملك', 'بريطاني', 'لا', 'خطب', 'بلغة', 'شوارع', 'أدان', '..', 'كل', 'مقام', 'مقال']\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"هل تحتاج إلى ترجمة كي تفهم خطاب الملك؟ اللغة \"الكلاسيكية\" (الفصحى) موجودة في كل اللغات وكذلك اللغة \"الدارجة\" .. الفرنسية التي ندرس في المدرسة ليست الفرنسية التي يستخدمها الناس في شوارع باريس .. وملكة بريطانيا لا تخطب بلغة شوارع لندن .. لكل مقام مقال\"\"\"\n",
        "lemmas = lemmer.lemmatize_text(text)\n",
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8101fb8d",
      "metadata": {
        "id": "8101fb8d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8fe57d5",
      "metadata": {
        "id": "c8fe57d5",
        "outputId": "fe170c62-9546-476c-ae93-47198ace7e9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('هل', 'stopword'), ('احتاج', 'verb'), ('إلى', 'stopword'), ('ترجمة', 'noun'), ('كي', 'stopword'), ('تف', 'noun'), ('خطاب', 'noun'), ('ملك', 'noun'), '؟', ('لغة', 'noun'), '\"', ('كلاسيكي', 'noun'), '\"(', ('فصحى', 'noun'), ')', ('موجود', 'noun'), ('في', 'stopword'), ('كل', 'stopword'), ('لغة', 'noun'), ('ذلك', 'stopword'), ('لغة', 'noun'), '\"', ('دارج', 'noun'), '\"..', ('فرنسة', 'noun'), ('التي', 'stopword'), ('درس', 'verb'), ('في', 'stopword'), ('مدرس', 'noun'), ('ليست', 'stopword'), ('فرنسة', 'noun'), ('التي', 'stopword'), ('استخدم', 'verb'), ('ناس', 'noun'), ('في', 'stopword'), ('شوارع', 'noun'), ('باريس', 'all'), '..', ('ملك', 'noun'), ('بريطاني', 'noun'), ('لا', 'stopword'), ('خطب', 'verb'), ('بلغة', 'noun'), ('شوارع', 'noun'), ('أدان', 'verb'), '..', ('كل', 'stopword'), ('مقام', 'noun'), ('مقال', 'noun')]\n"
          ]
        }
      ],
      "source": [
        "lemmas = lemmer.lemmatize_text(text, return_pos=True)\n",
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3763ea",
      "metadata": {
        "id": "cc3763ea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1143a9c1",
      "metadata": {
        "id": "1143a9c1",
        "outputId": "217d9011-3148-4fee-884e-e9f108304475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['هَلْ', 'اِحْتَاجَ', 'إِلَى', 'تَرْجَمَةٌ', 'كَيْ', 'تَفَهُّمٌ', 'خَطَّابٌ', 'مَلَكٌ', '؟', 'لُغَةٌ', '\"', 'كِلاَسِيكِيٌّ', '\"(', 'فُصْحَى', ')', 'مَوْجُودٌ', 'فِي', 'كُلَّ', 'لُغَةٌ', 'ذَلِكَ', 'لُغَةٌ', '\"', 'دَارِجٌ', '\"..', 'فَرَنْسِيّ', 'الَّتِي', 'دَرَسَ', 'فِي', 'مَدْرَسَةٌ', 'لَيْسَتْ', 'فَرَنْسِيّ', 'الَّتِي', 'اِسْتَخْدَمَ', 'نَاسٌ', 'فِي', 'شَوَارِعٌ', 'باريس', '..', 'مَلَكٌ', 'برِيطانِيا', 'لَا', 'خَطَبَ', 'بَلَغَةٌ', 'شَوَارِعٌ', 'أَدَانَ', '..', 'كُلَّ', 'مَقَامٌ', 'مَقَالٌ']\n"
          ]
        }
      ],
      "source": [
        "# put diacritics(التشكيل) on text\n",
        "lemmer.set_vocalized_lemma()\n",
        "lemmas = lemmer.lemmatize_text(text)\n",
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1f6cad2",
      "metadata": {
        "id": "c1f6cad2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "094fdb2f",
      "metadata": {
        "id": "094fdb2f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c89f0148",
      "metadata": {
        "id": "c89f0148"
      },
      "source": [
        "<b>\n",
        "<a id='special arabic cleaning functions'></a>\n",
        "<font size=\"5\">special arabic cleaning functions</font>\n",
        "</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0bc8dd4",
      "metadata": {
        "id": "c0bc8dd4",
        "outputId": "08aa6748-ba7d-4f2c-8a19-d4530dce05f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "هَلْ اِحْتَاجَ إِلَى تَرْجَمَةٌ كَيْ تَفَهُّمٌ خَطَّابٌ مَلَكٌ ؟ لُغَةٌ \" كِلاَسِيكِيٌّ \"( فُصْحَى ) مَوْجُودٌ فِي كُلَّ لُغَةٌ ذَلِكَ لُغَةٌ \" دَارِجٌ \".. فَرَنْسِيّ الَّتِي دَرَسَ فِي مَدْرَسَةٌ لَيْسَتْ فَرَنْسِيّ الَّتِي اِسْتَخْدَمَ نَاسٌ فِي شَوَارِعٌ باريس .. مَلَكٌ برِيطانِيا لَا خَطَبَ بَلَغَةٌ شَوَارِعٌ أَدَانَ .. كُلَّ مَقَامٌ مَقَالٌ\n",
            "-----------------------------------------------------\n",
            "هل احتاج إلى ترجمة كي تفهم خطاب ملك ؟ لغة \" كلاسيكي \"( فصحى ) موجود في كل لغة ذلك لغة \" دارج \".. فرنسي التي درس في مدرسة ليست فرنسي التي استخدم ناس في شوارع باريس .. ملك بريطانيا لا خطب بلغة شوارع أدان .. كل مقام مقال\n",
            "-----------------------------------------------------\n",
            "هل احتاج الي ترجمه كي تفهم خطاب ملك ؟ لغه \" كلاسيكي \"( فصحي ) موجود في كل لغه ذلك لغه \" دارج \".. فرنسي التي درس في مدرسه ليست فرنسي التي استخدم ناس في شوارع باريس .. ملك بريطانيا لا خطب بلغه شوارع ادان .. كل مقام مقال\n"
          ]
        }
      ],
      "source": [
        "# special cleaning to arabic text\n",
        "import re\n",
        "arabic_diacritics = re.compile(\"\"\"\n",
        "                             ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "\n",
        "\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_diacritics(text):\n",
        "    text = re.sub(arabic_diacritics, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "\n",
        "print(' '.join(lemmas))\n",
        "print('-----------------------------------------------------')\n",
        "print(remove_diacritics(' '.join(lemmas)))\n",
        "print('-----------------------------------------------------')\n",
        "print(normalize_arabic(remove_diacritics(' '.join(lemmas))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46cc9126",
      "metadata": {
        "id": "46cc9126"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2d2e78b",
      "metadata": {
        "id": "f2d2e78b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}